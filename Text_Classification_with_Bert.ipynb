{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextClassification with Bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8nju2e7Em9A",
        "outputId": "2a0c2d6f-57ce-48fc-be89-97907b3de97b"
      },
      "source": [
        "!pip install nltk numpy openpyxl Pillow pyparsing scikit-learn scipy tqdm keras scipy tensorflow transformers tensorflow-text tf-models-official"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (2.5.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (2.4.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.1MB 11.7MB/s \n",
            "\u001b[?25hCollecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/c0/c0fed4301f592c3b56638ae7292612c17d91a43891ba1aaf9636d535beae/tensorflow_text-2.4.3-cp37-cp37m-manylinux1_x86_64.whl (3.4MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.4MB 42.4MB/s \n",
            "\u001b[?25hCollecting tf-models-official\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/4a/23a08f8fd2747867ee223612e219eeb0d11c36116601d99b55ef3c72e707/tf_models_official-2.4.0-py2.py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 70.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.0.1)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 68.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 49.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.21.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (3.2.2)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.4.0)\n",
            "Collecting opencv-python-headless\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/6d/92f377bece9b0ec9c893081dbe073a65b38d7ac12ef572b8f70554d08760/opencv_python_headless-4.5.1.48-cp37-cp37m-manylinux2014_x86_64.whl (37.6MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37.6MB 90kB/s \n",
            "\u001b[?25hRequirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.5.12)\n",
            "Collecting tensorflow-model-optimization>=0.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/38/4fd48ea1bfcb0b6e36d949025200426fe9c3a8bfae029f0973d85518fa5a/tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 174kB 74.4MB/s \n",
            "\u001b[?25hCollecting py-cpuinfo>=3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/ba/77120e44cbe9719152415b97d5bfb29f4053ee987d6cb63f55ce7d50fadc/py-cpuinfo-8.0.0.tar.gz (99kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.12.8)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (4.0.1)\n",
            "Collecting tensorflow-addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 706kB 60.6MB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.1.5)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (4.1.3)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (5.4.8)\n",
            "Collecting dataclasses\n",
            "  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n",
            "Collecting tf-slim>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 358kB 66.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.29.22)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 55.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow) (56.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.28.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (1.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official) (1.3.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (4.0.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.6)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.26.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.17.4)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (0.3.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (20.3.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (0.29.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (0.16.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (5.1.2)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official) (2.7.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.22.0->tf-models-official) (2018.9)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tf-models-official) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tf-models-official) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tf-models-official) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (1.53.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Building wheels for collected packages: py-cpuinfo, seqeval\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-cp37-none-any.whl size=22245 sha256=f64d17b2f2aa9c25ea7d830fb0385c2e122e15f2524a6925712b263a02898df4\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/15/f5/aa2a056d223903b52cf4870134e3a01df0c723816835dd08db\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=70243b0257ace0dc4683cd5a25afd2232af0dfe49f5625e8c6c47b980f12bda7\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built py-cpuinfo seqeval\n",
            "\u001b[31mERROR: tf-models-official 2.4.0 has requirement pyyaml>=5.1, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, tensorflow-text, opencv-python-headless, tensorflow-model-optimization, py-cpuinfo, tensorflow-addons, seqeval, dataclasses, tf-slim, sentencepiece, tf-models-official\n",
            "Successfully installed dataclasses-0.6 opencv-python-headless-4.5.1.48 py-cpuinfo-8.0.0 sacremoses-0.0.45 sentencepiece-0.1.95 seqeval-1.2.2 tensorflow-addons-0.12.1 tensorflow-model-optimization-0.5.0 tensorflow-text-2.4.3 tf-models-official-2.4.0 tf-slim-1.1.0 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmHZBOJZHI2y",
        "outputId": "0f9a673b-0ad6-4445-9df8-4693e21ccffc"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "from openpyxl import load_workbook\n",
        "from sklearn import preprocessing\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "# K Fold validation with the model\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from keras.utils import to_categorical\n",
        "from transformers import BertTokenizer, TFBertModel, TFBertPreTrainedModel, TFBertMainLayer, TFBertForSequenceClassification\n",
        "\n",
        "\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sCzONTvwMOp"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtqFmQwwGuMo"
      },
      "source": [
        "contractions = [\n",
        "    (r\"won\\'t\", \"will not\"),\n",
        "    (r\"can\\'t\", \"cannot\"),\n",
        "    (r\"i\\'m\", \"i am\"),\n",
        "    (r\"ain\\'t\", \"is not\"),\n",
        "    (r\"(\\w+)\\'ll\", \"\\g<1> will\"),\n",
        "    (r\"(\\w+)n\\'t\", \"\\g<1> not\"),\n",
        "    (r\"(\\w+)\\'ve\", \"\\g<1> have\"),\n",
        "    (r\"(\\w+)\\'s\", \"\\g<1> is\"),\n",
        "    (r\"(\\w+)\\'re\", \"\\g<1> are\"),\n",
        "    (r\"(\\w+)\\'d\", \"\\g<1> would\")\n",
        "]\n",
        "\n",
        "\n",
        "class Expander(object):\n",
        "    def __init__(self, patterns=contractions):\n",
        "        self.patterns = [(re.compile(regex, re.IGNORECASE), repl) for (regex, repl) in patterns]\n",
        "\n",
        "    def expand(self, text):\n",
        "        s = text\n",
        "        for (pattern, repl) in self.patterns:\n",
        "            s = re.sub(pattern, repl, s)\n",
        "        return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57rrd0f2FQRT",
        "outputId": "dcb58ef9-e9b8-407d-c68c-88a29f2792ba"
      },
      "source": [
        "ALL_CLASSES = [\n",
        "    \"content discussion\", \"greeting\", \"logistics\", \"assignment instructions\", \"instruction question\",\n",
        "    \"assignment question\", \"general comment\", \"response\", \"incomplete/typo\", \"feedback\",\n",
        "    \"emoticon/non-verbal\", \"discussion wrap-up\", \"outside material\", \"opening statement\",\n",
        "    \"general question\", \"content question\", \"general discussion\"\n",
        "]\n",
        "DATASET_PATH = \"data/dataset.xlsx\"\n",
        "\n",
        "# Opens a single sheet for reading, returns a list of messages and a list of classes\n",
        "def read_worksheet(filename, sheet_name, all_classes, label_encoder, no_columns):\n",
        "    wb = load_workbook(filename, read_only=True)\n",
        "    ws = wb[sheet_name]\n",
        "    column_labels = next(ws.rows)\n",
        "    X = []\n",
        "    y = []\n",
        "    for row in ws.rows:\n",
        "        if row[0].value is None:\n",
        "            break\n",
        "        elif row[0].value.strip() == \"Course\":  # Skip the first line which only contains column titles\n",
        "            continue\n",
        "        new_entry = {}\n",
        "        for i in range(no_columns):\n",
        "            new_entry[column_labels[i].value.lower().replace(\" \", \"_\")] = str(row[i].value)\n",
        "        c_list = [new_entry[\"codepreliminary\"].lower().strip()]\n",
        "        if c_list[0] not in all_classes:\n",
        "            c_list = new_entry[\"codepreliminary\"].lower().strip().split(\"/\")\n",
        "        # If there are 2 classes listed in document add message twice (1 for each class)\n",
        "        for c in c_list:\n",
        "            new_entry[\"codepreliminary\"] = label_encoder.transform([c])[0]\n",
        "            X.append(new_entry[\"message\"])\n",
        "            y.append(label_encoder.transform([c])[0])\n",
        "    wb.close()\n",
        "    return X, y\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(ALL_CLASSES)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ4DTlyTFvyt"
      },
      "source": [
        "def preprocess_data(dataset, classes):\n",
        "    # List of emojis found in messages\n",
        "    emojis = [\":)\", \":(\", \":D\", \"ðŸ‘\"]\n",
        "    ps = PorterStemmer()\n",
        "    tkn = TweetTokenizer()  # Use tweet tokenizer to not split emojis as punctuation\n",
        "    exp = Expander()  # Expands contractions such as I'm, he's into I am, he is\n",
        "    new_dataset = []\n",
        "    new_classes = []\n",
        "    stopword_set = stopwords.words(\"english\")\n",
        "    for i, entry in enumerate(dataset):\n",
        "        exp.expand(entry)\n",
        "        new_entry = []\n",
        "        # Remove capitalization, stopwords\n",
        "        for token in tkn.tokenize(entry):\n",
        "            new_token = ps.stem(token)  # Stem the token and convert to lowercase\n",
        "            if new_token.startswith(\"http\"):\n",
        "                new_token = \"url\"  # Replace links with a url tag\n",
        "            if any(emoji in new_token for emoji in emojis):\n",
        "                new_token = \"emoji\"  # Replace emojis with a string\n",
        "            if (new_token not in stopword_set or len(new_token) == 1) and new_token.isalnum():\n",
        "                new_entry.append(new_token)\n",
        "        if len(new_entry) > 0:\n",
        "            new_dataset.append(\" \".join(new_entry))\n",
        "            new_classes.append(classes[i])\n",
        "\n",
        "    return new_dataset, new_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mao_TlHWGgdp"
      },
      "source": [
        "X, y = read_worksheet(DATASET_PATH, \"Discussion only data\", ALL_CLASSES, le, 10)\n",
        "X2, y2 = read_worksheet(DATASET_PATH, \"CREW data\", ALL_CLASSES, le, 11)\n",
        "X.extend(X2)  # Join worksheets into a single dataset\n",
        "y.extend(y2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55lFZpUcyXz9"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 32\n",
        "random_seed = 99\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((tf.constant(X_train), tf.constant(y_train))).batch(batch_size)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((tf.constant(X_test), tf.constant(y_test))).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0FSY7_dyWkt",
        "outputId": "dd9a468e-e7bb-44f0-9a06-42a0d31e5e21"
      },
      "source": [
        "bert_preprocess_model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\", name=\"preprocessing\")\n",
        "bert_model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\", name=\"Bert_encoder\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:7 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x7f3391398cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:7 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x7f3391398cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:8 out of the last 9 calls to <function recreate_function.<locals>.restored_function_body at 0x7f33913a03b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:8 out of the last 9 calls to <function recreate_function.<locals>.restored_function_body at 0x7f33913a03b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpkZpKtQyhYi"
      },
      "source": [
        "def build_classifier_model(preprocess_model, bert_model, no_classes):\n",
        "  text_input = layers.Input(shape=(), dtype=tf.string, name=\"text\")\n",
        "  preprocessing_layer = preprocess_model\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = bert_model\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs[\"pooled_output\"]\n",
        "  net = layers.Dropout(0.1)(net)\n",
        "  net = layers.Dense(no_classes, activation=\"relu\", name=\"classifier\")(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEl2is25zFS7",
        "outputId": "3d2f03b4-456a-4edc-a1cc-a6811ee9599a"
      },
      "source": [
        "model = build_classifier_model(bert_preprocess_model, bert_model, len(ALL_CLASSES))\n",
        "#tf.keras.utils.plot_model(model)\n",
        "epochs = 5\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_data).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "optimizer = optimization.create_optimizer(init_lr=3e-5,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metric)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "NuEJNQwFz82y",
        "outputId": "b0f90c84-c9bf-4c02-82cb-03415d029f9a"
      },
      "source": [
        "history = model.fit(x=train_data, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            " 3/22 [===>..........................] - ETA: 33s - loss: 9.6475 - accuracy: 0.0642"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-2814ea848a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5duBsjv4JGL2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "5df3744a-1f2a-4bcf-927b-4a8ce0162e79"
      },
      "source": [
        "# Plot history\n",
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "\n",
        "acc = history_dict['accuracy']\n",
        "loss = history_dict['loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f338c43ff10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAGDCAYAAABuj7cYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd5hU9fn+8fcDLL0pYKMTwQIKyAoCEbFFjDVRI2pQowaxRKNGU41Ikl+aX2OMmtijiYotEuwNEQsoixIVREUFWQVFepH+/P74nHVmh9ndYdmZMztzv65rLmbOOXPmOTu63HzaMXdHRERERPJDg7gLEBEREZEEhTMRERGRPKJwJiIiIpJHFM5ERERE8ojCmYiIiEgeUTgTERERySMKZyJSIzN70szOqOtj42Rm88zssCyc181s9+j5P8zsykyOrcXnnGZmz9S2zmrOO9zMyuv6vCKSuUZxFyAi2WFmq5NeNgfWA5uj1+e6+z2Znsvdj8zGsYXO3cfUxXnMrBvwMVDi7puic98DZPwdikj9oXAmUqDcvWXFczObB5zj7s+lHmdmjSr+whcRkfipW1OkyFR0W5nZT81sEXCnme1gZo+Z2WIzWxY975T0nslmdk70/Ewze9nMromO/djMjqzlsd3NbIqZrTKz58zsRjP7dxV1Z1Ljb8zsleh8z5hZ+6T9o8xsvpktMbNfVvPzGWRmi8ysYdK275jZW9HzgWY21cyWm9lCM7vBzBpXca5/mtlvk15fHr3nMzM7K+XYo8zsTTNbaWYLzGxs0u4p0Z/LzWy1mQ2u+NkmvX+ImU03sxXRn0My/dlUx8z2it6/3MxmmdmxSfu+bWazo3N+amY/iba3j76f5Wa21MxeMjP9fSOSIf3PIlKcdgF2BLoCowm/C+6MXncBvgJuqOb9g4D3gPbAn4Dbzcxqcey9wOtAO2AsMKqaz8ykxlOBHwA7AY2BirCwN/D36Py7RZ/XiTTc/TVgDXBIynnvjZ5vBi6JrmcwcChwfjV1E9UwIqrncKAnkDrebQ1wOtAWOAo4z8yOj/YNi/5s6+4t3X1qyrl3BB4Hro+u7VrgcTNrl3INW/1saqi5BHgUeCZ634+Ae8xsj+iQ2wld5K2APsCkaPtlQDnQAdgZ+AWgewWKZEjhTKQ4bQGucvf17v6Vuy9x94fdfa27rwJ+BxxUzfvnu/ut7r4ZuAvYlfCXcMbHmlkXYH/g1+6+wd1fBiZW9YEZ1ninu7/v7l8BDwD9ou0nAo+5+xR3Xw9cGf0MqnIfcAqAmbUCvh1tw91nuPs0d9/k7vOAm9PUkc73ovrecfc1hDCafH2T3f1td9/i7m9Fn5fJeSGEuQ/c/V9RXfcBc4Bjko6p6mdTnQOAlsAfou9oEvAY0c8G2AjsbWat3X2Zu7+RtH1XoKu7b3T3l1w3chbJmMKZSHFa7O7rKl6YWXMzuznq9ltJ6EZrm9y1l2JRxRN3Xxs9bbmNx+4GLE3aBrCgqoIzrHFR0vO1STXtlnzuKBwtqeqzCK1k3zWzJsB3gTfcfX5UR6+oy25RVMf/I7Si1aRSDcD8lOsbZGYvRN22K4AxGZ634tzzU7bNBzomva7qZ1Njze6eHGSTz3sCIbjON7MXzWxwtP3PwFzgGTP7yMx+ltlliAgonIkUq9RWjMuAPYBB7t6aRDdaVV2VdWEhsKOZNU/a1rma47enxoXJ544+s11VB7v7bEIIOZLKXZoQukfnAD2jOn5RmxoIXbPJ7iW0HHZ29zbAP5LOW1Or02eE7t5kXYBPM6irpvN2Thkv9vV53X26ux9H6PKcQGiRw91Xuftl7t4DOBa41MwO3c5aRIqGwpmIALQijOFaHo1fuirbHxi1RJUBY82scdTqckw1b9meGh8Cjjazb0aD98dR8++/e4GLCSHwwZQ6VgKrzWxP4LwMa3gAONPM9o7CYWr9rQgtievMbCAhFFZYTOiG7VHFuZ8AepnZqWbWyMxOBvYmdEFuj9cIrWxXmFmJmQ0nfEfjo+/sNDNr4+4bCT+TLQBmdrSZ7R6NLVxBGKdXXTeyiCRROBMRgOuAZsCXwDTgqRx97mmEQfVLgN8C9xPWY0un1jW6+yzgAkLgWggsIwxYr07FmK9J7v5l0vafEILTKuDWqOZMangyuoZJhC6/SSmHnA+MM7NVwK+JWqGi964ljLF7JZoBeUDKuZcARxNaF5cAVwBHp9S9zdx9AyGMHUn4ud8EnO7uc6JDRgHzou7dMYTvE8KEh+eA1cBU4CZ3f2F7ahEpJqYxmiKSL8zsfmCOu2e95U5EJF+p5UxEYmNm+5vZN8ysQbTUxHGEsUsiIkVLdwgQkTjtAvyHMDi/HDjP3d+MtyQRkXipW1NEREQkj6hbU0RERCSPKJyJiIiI5JGCGXPWvn1779atW9xliIiIiNRoxowZX7p7h3T7CiacdevWjbKysrjLEBEREamRmaXecu1r6tYUERERySMKZyIiIiJ5ROFMREREJI8UzJiznLj5Zjj6aOjYMe5KREREarRx40bKy8tZt25d3KUUraZNm9KpUydKSkoyfo/CWabmzYPzz4cLL4RTToHLLoO+feOuSkREpErl5eW0atWKbt26YWZxl1N03J0lS5ZQXl5O9+7dM36fujUz1a0bzJ0bAtp//gP9+sFhh8GTT4LusiAiInlo3bp1tGvXTsEsJmZGu3bttrnlUuFsW3TvDn/9KyxYAH/4A7z7Lnz729CnD9x+O6jZWERE8oyCWbxq8/NXOKuNHXaAn/4UPv4Y7r4bSkrgnHOga1f4zW/gyy/jrlBERCR2S5YsoV+/fvTr149ddtmFjh07fv16w4YN1b63rKyMiy66qMbPGDJkSJ3UOnnyZI4++ug6Odf2UjjbHo0bw6hR8Oab8NxzMGAA/PrX0KULnHcevP9+3BWKiIjEpl27dsycOZOZM2cyZswYLrnkkq9fN27cmE2bNlX53tLSUq6//voaP+PVV1+ty5LzgsJZXTCDQw+FJ56AWbPg1FPhjjtgzz3huOPgxRc1Lk1ERAQ488wzGTNmDIMGDeKKK67g9ddfZ/DgwfTv358hQ4bw3nvvAZVbssaOHctZZ53F8OHD6dGjR6XQ1rJly6+PHz58OCeeeCJ77rknp512Gh793fvEE0+w5557MmDAAC666KIaW8iWLl3K8ccfz7777ssBBxzAW2+9BcCLL774dctf//79WbVqFQsXLmTYsGH069ePPn368NJLL233z0izNeva3nvDbbfB734HN94IN90EEydCaSlceimceGLoBhUREcmlH/8YZs6s23P26wfXXbfNbysvL+fVV1+lYcOGrFy5kpdeeolGjRrx3HPP8Ytf/IKHH354q/fMmTOHF154gVWrVrHHHntw3nnnbbU8xZtvvsmsWbPYbbfdGDp0KK+88gqlpaWce+65TJkyhe7du3PKKafUWN9VV11F//79mTBhApMmTeL0009n5syZXHPNNdx4440MHTqU1atX07RpU2655RaOOOIIfvnLX7J582bWrl27zT+PVGo5y5add4Zx4+CTT+Dvf4eVK0OL2u67w7XXhtciIiJF6KSTTqJhw4YArFixgpNOOok+ffpwySWXMGvWrLTvOeqoo2jSpAnt27dnp5124vPPP9/qmIEDB9KpUycaNGhAv379mDdvHnPmzKFHjx5fL2WRSTh7+eWXGTVqFACHHHIIS5YsYeXKlQwdOpRLL72U66+/nuXLl9OoUSP2339/7rzzTsaOHcvbb79Nq1atavtj+ZpazrKteXMYMwZGj4bHHoP/+7+wRtrVV8MPfwgXXRTGqImIiGRTLVq4sqVFixZfP7/yyis5+OCDeeSRR5g3bx7Dhw9P+54mTZp8/bxhw4Zpx6tlcsz2+NnPfsZRRx3FE088wdChQ3n66acZNmwYU6ZM4fHHH+fMM8/k0ksv5fTTT9+uz1HLWa40aADHHhvGn02fHpbguO466NEjtKjNmBF3hSIiIjm3YsUKOkZ33vnnP/9Z5+ffY489+Oijj5g3bx4A999/f43vOfDAA7nnnnuAMJatffv2tG7dmg8//JB99tmHn/70p+y///7MmTOH+fPns/POO/PDH/6Qc845hzfeeGO7a855ODOzpmb2upn9z8xmmdnVaY4508wWm9nM6HFOruvMqtJSuO8++OgjuPji0KJWWgrDh8Ojj8KWLXFXKCIikhNXXHEFP//5z+nfv3+dt3QBNGvWjJtuuokRI0YwYMAAWrVqRZs2bap9z9ixY5kxYwb77rsvP/vZz7jrrrsAuO666+jTpw/77rsvJSUlHHnkkUyePJm+ffvSv39/7r//fi6++OLtrtk8x7MILazG1sLdV5tZCfAycLG7T0s65kyg1N0vzPS8paWlXlZWVuf15sSKFWESQcUCt3vsAZdcAqefDs2axV2diIjUU++++y577bVX3GXEbvXq1bRs2RJ354ILLqBnz55ccsklOfv8dN+Dmc1w99J0x+e85cyD1dHLkuhR3OtMtGkTxqF9+GFoUWvVKoxT69IFrroKvvgi7gpFRETqrVtvvZV+/frRu3dvVqxYwbnnnht3SdWKZcyZmTU0s5nAF8Cz7v5amsNOMLO3zOwhM+uc4xLjUVICI0fC66+HsWlDhoQ7DnTpEiYPzJ4dd4UiIiL1TsXit7Nnz+aee+6hefPmcZdUrVjCmbtvdvd+QCdgoJn1STnkUaCbu+8LPAvcle48ZjbazMrMrGzx4sXZLTqXzGDYMPjvf2HOHPjBD+Df/4bevcNEguef16K2IiIiBSrW2Zruvhx4ARiRsn2Ju6+PXt4GDKji/be4e6m7l3bo0CG7xcalV6+wTtqCBWHdtBkz4LDDYL/94F//ghruTSYiIsUt12PLpbLa/PzjmK3ZwczaRs+bAYcDc1KO2TXp5bHAu7mrME+1bw9XXgnz58Ott8L69WHCQI8e8Mc/wvLlcVcoIiJ5pmnTpixZskQBLSbuzpIlS2jatOk2vS+O2Zr7EropGxLC4QPuPs7MxgFl7j7RzH5PCGWbgKXAee4+p8qTUs9na9bGli3w1FNhUdtJk6BFCzj77HB7jmgVZBERKW4bN26kvLycdevWxV1K0WratCmdOnXa6lZT1c3WzHk4y5aiC2fJZs4MIW38+BDavvvdMPvzgAPirkxERETSyKulNCQL+vUL488+/hh+8hN49lkYPBiGDoX//Ac2b467QhEREcmQwlkh6dQpjD8rLw8L2i5cCCecEBa1veEGWLMm7gpFRESkBgpnhahly3BD9fffhwcfhA4d4Ec/gs6d4Ze/DKFNRERE8pLCWSFr1AhOPBGmToVXXoGDD4bf/x66doUzz4S33467QhEREUmhcFYshgyBhx+GDz6Ac88NLWr77gvf+hY8/bQWtRUREckTCmfF5hvfgL/9LSxq+//+H7zzDowYEYLanXeG9dNEREQkNgpnxWrHHeHnPw8zPP/5z3DLqLPOgm7d4He/gyVL4q5QRESkKCmcFbsmTeCMM+B//4NnnoG+feFXvwo3W7/gApg7N+4KRUREiorCmQRmcPjh4a4Db78NJ58Mt90W7u35ne/Ayy9rXJqIiEgOKJzJ1vr0gTvugHnz4Be/gClT4MADwx0HHngANm2Ku0IREZGCpXAmVdt1V/jtb+GTT+DGG2Hp0tCi1rMnXHcdrFoVd4UiIiIFR+FMataiBZx/PsyZA488Eu5EcMklYVHbK64IdyQQERGROqFwJplr2BCOPx5eegleew2OOCLccL17d/j+9+HNN+OuUEREpN5TOJPaGTgQ7r8fPvwQLrwQ/vtf2G8/OOQQePxx2LIl7gpFRETqJYUz2T7dusFf/hIWtf3Tn8L9PI8+Gnr3hltvhXXr4q5QRESkXlE4k7rRti1cfnlY1Pbf/4ZmzWD06LBe2tVXw+LFcVcoIiJSLyicSd0qKYHTToMZM2DSpND9OXZsCGnnngvvvRd3hSIiInlN4UyywwwOPhgeewxmz4ZRo+Cuu2DPPeGYY2DyZC1qKyIikobCmWTfXnvBLbeE9dKuugqmTQvBrbQU7r0XNm6Mu0IREZG8oXAmubPTTqGL85NP4OabYc2a0AX6jW/ANdfAihVxVygiIhI7hTPJvYrJArNnw6OPhnB2+eVhUdtLL4X58+OuUEREJDYKZxKfBg3CshsvvABlZWEs2vXXh7A2ciRMnx53hSIiIjmncCb5YcAAuOeesBTHJZfAk0+GmZ7DhoUFbrWorYiIFAmFM8kvnTvDn/8cFrW99trQxXn88WGW59//DmvXxl2hiIhIVimcSX5q3Tq0oH34IYwfHxa5Pf/8sF7alVfCokVxVygiIpIVCmeS3xo1gpNPDjdanzIFvvlN+N3voGtXOPtsmDUr7gpFRETqlMKZ1A9mcOCBMGECzJkTgtl990GfPnDkkfDcc1rUVkRECoLCmdQ/vXrBTTeF9dJ+8xt48004/HDo1w/uvhs2bIi7QhERkVpTOJP6q317+NWvYN48uP122LwZzjgDuneHP/wBli2Lu0IREZFtpnAm9V/TpnDWWfD222EJjr33hp//PMz8vOgi+OijuCsUERHJmMKZFA4zGDECnn0WZs6EE06Af/wDevaEE0+EqVPjrlBERKRGCmdSmPr2hbvuCl2eV1wBzz8PQ4aEx8MPhy5QERGRPJTzcGZmTc3sdTP7n5nNMrOrqzn2BDNzMyvNZY1SQHbbDX7/+7Co7fXXh/XRTjwxTCr4299g9eq4KxQREakkjpaz9cAh7t4X6AeMMLMDUg8ys1bAxcBrOa5PClHLlvCjH8EHH8BDD8HOO4fxaJ07h/Fpn30Wd4UiIiJADOHMg4rmipLokW6Bqt8AfwTW5ao2KQING4axaK++Gh6HHgp/+hN06xZmev7vf3FXKCIiRS6WMWdm1tDMZgJfAM+6+2sp+/cDOrv743HUJ0Vi8ODQivbBBzBmTBiL1q9fWDPtqae0qK2IiMQilnDm7pvdvR/QCRhoZn0q9plZA+Ba4LKazmNmo82szMzKFi9enL2CpbD16BHGoy1YEManzZ4d7jqwzz5wxx2wfn3cFYqISBGJdbamuy8HXgBGJG1uBfQBJpvZPOAAYGK6SQHufou7l7p7aYcOHXJRshSyHXaAn/0MPv44zPRs2DDcJqprVxg3Dt55R61pIiKSdXHM1uxgZm2j582Aw4E5FfvdfYW7t3f3bu7eDZgGHOvuZbmuVYpU48Zw+ulhrbRnn4X+/eGqq0JL2q67wmmnwZ13httHiYiI1LFGMXzmrsBdZtaQEA4fcPfHzGwcUObuE2OoSWRrZnDYYeGxYEG4ufrzz4c/7703HNOzZ9h/6KFw8MGw447x1iwiIvWe+XZ005hZC+Ard99iZr2APYEn3X1jXRWYqdLSUi8rU+Oa5IA7zJqVCGqTJ4f10sxgv/0SgW7oUGjWLO5qRUQkD5nZDHdPu47r9oazGcCBwA7AK8B0YIO7n1brk9aSwpnEZuNGmD49BLXnnoNp08K2Jk1CQKtoWRswIIxjExGRopfNcPaGu+9nZj8Cmrn7n8xsZjQTM6cUziRvrF4NL72U6AatWDutbdvQ9XnooSGw9eoVWttERKToVBfOtnfMmZnZYOA04Oxom5oGpLi1bBmW4jjyyPD6iy9g0qQQ1J59Fh55JGzv1CkR1A49NEw2EBGRore94ezHwM+BR9x9lpn1ICyNISIVdtoJRo4MD3f46KPEeLXHHgvLdgDsvXciqB10ELRpE2/dIiISi+3q1qx0orB4bEt3X1knJ9xG6taUemnLltDtWdEFOmUKfPVVGJs2cGCiZe2AA8IYNhERKQjZHHN2LzAG2EyYDNAa+Ku7/7nWJ60lhTMpCOvXw9SpiZa1118PAa55czjwwETLWt++0CDWNaRFRGQ7ZDOczXT3fmZ2GrAf8DNghrvvW+uT1pLCmRSkFSvgxRcTM0HffTdsb98eDjkk0bLWo0e8dYqIyDbJ5oSAEjMrAY4HbnD3jWam+9uI1JU2beDYY8MD4LPPEq1qzz8PDzwQtnfvnghqhxwCup2ZiEi9tb3h7GZgHvA/YIqZdQViGXMmUhR22w1GjQoPd3jvvURYe/BBuO22cFzfvonFcA88EFq0iLduERHJWJ1NCPj6hGaN3H1TnZ40A+rWlKK3aRO88UaiC/SVV2DDBigpgcGDE+PV9t8/bBMRkdhkc8xZG+AqYFi06UVgnLuvqPVJa0nhTCTF2rUhoFV0gb7xRmhta9UKhg9PdIPuvbcWwxURybFshrOHgXeAaKEmRgF93f27tT5pLSmcidRgyRJ44YVEN+jcuWH7LrtUXgy3c+d46xQRKQJZn61Z07ZcUDgT2Ubz51eeXPDFF2F7r16JoHbwwbDDDvHWKSJSgKoLZ9u7UNJXZvbNpA8aCny1necUkVzo2hXOOgvuvRcWLYK33oJrr4Xddw93LTjhhLBkx8CB8POfhwC3bl3cVYuIFLztbTnrC9wNVNxnZhlwhru/VQe1bRO1nInUoQ0bwgK4Fa1q06aFCQdNm8LQoYmZoP37h7sZiIjINslat2bSB7QGcPeVZvZjd79uu0+6jRTORLJo1Sp46aXETNC33w7b27atvBhuz56aXCAikoGsh7OUD/vE3bvU6UkzoHAmkkOffw6TJiXC2iefhO2dO1eeXLDLLvHWKSKSp3Idzha4e86neymcicTEHT78MNEFOmkSLF0a9vXunQhqBx0ErVvHW6uISJ5Qy5mI5M7mzTBzZmIm6EsvhYkEDRvCoEGJlrUDDoDGjeOuVkQkFnUezsxsFZDujQY0c/ftvS3UNlM4E8lT69bB1KmJlrXp02HLFmjeHIYNS0wu2GcfaLC9E8hFROqHnLacxUXhTKSeWL4cJk9OtKzNmRO2t29febxa9+6xlikikk0KZyKSvz79NBHUnnsOFi4M23v0SAS1Qw4J4U1EpEAonIlI/eAeWtIqgtrkybByZdjXv3+iZe2b34QWLWItVURkeyiciUj9tGkTlJUlxqu9+mpYILekBIYMSbSs7b8/NMr5UFcRkVpTOBORwrB2Lbz8cqJlbebM0NrWujUMH55oWdtrLy2GKyJ5rbpwpn9qikj90bw5fOtb4QHw5ZfwwguJlrWJE8P2XXetPLmgU6f4ahYR2UZqORORwvHxxyGkVTwWLw7b99gjsWTH8OHhtlMiIjFSt6aIFJ8tW8I9QCtmgr74YugWbdAASksTLWtDhoQbuouI5JDCmYjIhg3w2muJ8WqvvRbuZtC0aZj9WdGy1q9fuJuBiEgWKZyJiKRauRKmTEm0rL3zTti+ww5hXbWKlrXdd9fkAhGpcwpnIiI1WbQo3LS9omVtwYKwfdddw4K4Xbtu/ejSReutiUitKJyJiGwLd5g7N4S0adNg/vzwKC8Pa68la98+hLR04a1rV9hxR7W8ichW8iqcmVlTYArQhLCUx0PuflXKMWOAC4DNwGpgtLvPru68CmciknWbN8NnnyXCWvLjk0/Cn2vXVn5PixZVt7p17Rpa5jTGTaTo5Fs4M6CFu682sxLgZeBid5+WdExrd18ZPT8WON/dR1R3XoUzEYmdOyxZkj68VTyWLq38npKSsA5bVS1vnTtDkybxXI+IZE1eLULrIQ2ujl6WRA9POWZl0ssWqftFRPKSWejmbN8eBgxIf8zq1ZVb2pIfzz0XWuZS/9G8666VW9tSH61bZ//aRCRnYrlDgJk1BGYAuwM3uvtraY65ALgUaAwcktsKRUSypGVL6N07PNLZsCGMbUvXbfrGGzBhQjgmWdu2Vbe8dekCO+2kcW8i9UisEwLMrC3wCPAjd3+nimNOBY5w9zPS7BsNjAbo0qXLgPnz52ezXBGR+G3ZAp9/Xn3X6apVld/TtGn1kxY6dtSN40VyLK/GnG1VgNmvgbXufk0V+xsAy9y9TXXn0ZgzERFCl+jy5VV3nc6fD198Ufk9DRuGgJY6WSH5dfPm8VyPSIHKqzFnZtYB2Ojuy82sGXA48MeUY3q6+wfRy6OADxARkZqZhYV0d9gh3O0gna++qjq4vfRS6FbdvLnyezp0qLrlrWvX0LWqrlOROhFHO/auwF3RuLMGwAPu/piZjQPK3H0icKGZHQZsBJYBW3VpiohILTVrFm4Gv8ce6fdv2lT1kiHvvANPPBECXrKWLasPb7vsEu5rKiI1ir1bs66oW1NEJEfcYfHi6rtOly2r/J7GjcOyIFV1nXbuHI4RKRJ51a0pIiL1nFmYAbrTTrD//umPWbWq6sV6n34aFi6svGSIWWLJkKoeLVvm5vpEYqZwJiIida9VK+jTJzzSWb8+/ZIh8+fD66/Dww/Dxo2V37PDDtWHt/btNe5NCoLCmYiI5F6TJvCNb4RHOlu2hNa1dN2mc+fC88+HBX2TNW+efqZpxfPddtOSIVIv6L9SERHJPw0ahOU9OnaEwYO33u8exrVV1XX6xhthXFyyhg2rv1VWly5hTTiRmCmciYhI/WMGO+4YHv37pz9m7dqqJyxMngyffhpa6JLtvHOitW2PPWDIkBAOd9gh65ckUkHhTEREClPz5rDnnuGRzsaNIaClC3BvvQWPPJJY723vvWHo0BDWhg6F3XfX+DbJGoUzEREpTiUl0K1beKSzZg1Mnw6vvgqvvAIPPgi33hr2degQglpFWBswQF2iUmcUzkRERNJp0QKGDw8PCF2gc+Ykwtqrr8J//xv2NW4cAlpFWBsyJHSRitSCFqEVERGprcWLYerURFibPj0sEwJhJmpyWOvdW3dJkK/l9Y3P64rCmYiIxG79enjzzURYe+UV+PzzsK9NGzjggERYGzRIC+sWMYUzERGROLjDRx9V7gp9552wvUED6Nu38kSDzp010aBIKJyJiIjki+XL4bXXEmFt2rQw+QDCum7JYa1v3zBxQQqOwpmIiEi+2rQJ3n67clfoJ5+Efc2bw8CBibB2wAFhbTep9xTORERE6pPy8hDUKsLam29WXnMteaJBz57qCq2HFM5ERETqs9Q11159NXSPQrjhe3JYKy3Vmmv1QHXhTOuciYiI5LtM1lybODHsKykJa65VhLUhQ2CXXeKqXGpBLWciIiKFoLo113r0qDzRYO+9w/UZ0qAAACAASURBVI3gJTbq1hQRESk21a251rp1uKF7RVgbOBBatYq33iKjcCYiIlLs3OHjjyuHtdQ115LHrnXpookGWaRwJiIiIltbsSKss1YR1lLXXEsOa/36ac21OqQJASIiIrK1Nm3giCPCAxJrriVPNHjwwbCvWbPQ/VkR1gYP1pprWaKWMxEREalaeXliokHqmmt77VV5ooHWXMuYujVFRESkbqSuuTZ1KixbFvZVrLlWEda05lqV1K0pIiIidaM2a64lj13Tmms1UsuZiIiI1K2a1lxLDmu9exflmmvq1hQREZH41LTm2gEHJMLaoEFFseaawpmIiIjkj5rWXNt338oTDQpwzTWFMxEREclv1a25tttulcNaAay5pgkBIiIikt9qs+ZaRVgrsDXX1HImIiIi9UNNa64lTzTo1Suvu0LVrSkiIiKFp7o119q1qxzWSktDi1ueULemiIiIFJ5M1lx79NGwr6QE9tuv8ti1PF1zLectZ2bWFJgCNCGEw4fc/aqUYy4FzgE2AYuBs9x9fnXnVcuZiIiIbKW6Nde6d68c1nK45lpedWuamQEt3H21mZUALwMXu/u0pGMOBl5z97Vmdh4w3N1Pru68CmciIiJSo0zWXDvmGLjwwqyWUV04a5DVT07Dg9XRy5Lo4SnHvODua6OX04BOOSxRREREClWTJiGAXXYZPPwwLFwIH34Id98Np54aXk+dGmuJsYw5M7OGwAxgd+BGd3+tmsPPBp7MSWEiIiJSXMzCLaV69IBRo8K2LVtiLSnnLWcA7r7Z3fsRWsQGmlmfdMeZ2feBUuDPVewfbWZlZla2ePHi7BUsIiIixaNBLPEo8fFxfri7LwdeAEak7jOzw4BfAse6+/oq3n+Lu5e6e2mHDh2yW6yIiIhIDuQ8nJlZBzNrGz1vBhwOzEk5pj9wMyGYfZHrGkVERETiEseYs12Bu6JxZw2AB9z9MTMbB5S5+0RCN2ZL4MEwuZNP3P3YGGoVERERyamchzN3fwvon2b7r5OeH5bTokRERETyRMHcvsnMFgPVLlRbR9oDX+bgc/JRMV87FPf169qLVzFffzFfOxT39efi2ru6e9oB8wUTznLFzMqqWjSu0BXztUNxX7+uvTivHYr7+ov52qG4rz/ua493rqiIiIiIVKJwJiIiIpJHFM623S1xFxCjYr52KO7r17UXr2K+/mK+diju64/12jXmTERERCSPqOVMREREJI8onKVhZneY2Rdm9k4V+83MrjezuWb2lpntl+sasyWDax9uZivMbGb0+HW64+ojM+tsZi+Y2Wwzm2VmF6c5ppC/+0yuvyC/fzNramavm9n/omu/Os0xTczs/ui7f83MuuW+0uzI8PrPNLPFSd/9OXHUmi1m1tDM3jSzx9LsK9jvHmq89kL/3ueZ2dvRtZWl2R/L7/w47hBQH/wTuAG4u4r9RwI9o8cg4O/Rn4Xgn1R/7QAvufvRuSknpzYBl7n7G2bWCphhZs+6++ykYwr5u8/k+qEwv//1wCHuvtrMSoCXzexJd5+WdMzZwDJ3393MRgJ/BE6Oo9gsyOT6Ae539wtjqC8XLgbeBVqn2VfI3z1Uf+1Q2N87wMHuXtWaZrH8zlfLWRruPgVYWs0hxwF3ezANaGtmu+amuuzK4NoLlrsvdPc3ouerCL+sOqYcVsjffSbXX5Ci73N19LIkeqQOyD0OuCt6/hBwqEX3l6vvMrz+gmVmnYCjgNuqOKRgv/sMrr3YxfI7X+GsdjoCC5Jel1Mkf4lFBkfdH0+aWe+4i8mGqNuiP/Bayq6i+O6ruX4o0O8/6tqZCXwBPOvuVX737r4JWAG0y22V2ZPB9QOcEHXtPGRmnXNcYjZdB1wBbKlifyF/9zVdOxTu9w7hHyHPmNkMMxudZn8sv/MVzmRbvUG45URf4G/AhJjrqXNm1hJ4GPixu6+Mu55cq+H6C/b7d/fN7t4P6AQMNLM+cdeUSxlc/6NAN3ffF3iWREtSvWZmRwNfuPuMuGvJtQyvvSC/9yTfdPf9CN2XF5jZsLgLAoWz2voUSP7XQ6doW8Fz95UV3R/u/gRQYmbtYy6rzkTjbR4G7nH3/6Q5pKC/+5quv9C/fwB3Xw68AIxI2fX1d29mjYA2wJLcVpd9VV2/uy9x9/XRy9uAAbmuLUuGAsea2TxgPHCImf075ZhC/e5rvPYC/t4BcPdPoz+/AB4BBqYcEsvvfIWz2pkInB7N4jgAWOHuC+MuKhfMbJeKsRZmNpDw31Ah/JIiuq7bgXfd/doqDivY7z6T6y/U79/MOphZ2+h5M+BwYE7KYROBM6LnJwKTvEAWiszk+lPG2RxLGJNY77n7z929k7t3A0YSvtfvpxxWkN99JtdeqN87gJm1iCY/YWYtgG8BqSsVxPI7X7M10zCz+4DhQHszKweuIgyQxd3/ATwBfBuYC6wFfhBPpXUvg2s/ETjPzDYBXwEjC+GXVGQoMAp4Oxp7A/ALoAvU7+/ezJ4Exrt7dV0SFde/ysy+Ayxj6+vPm+8/+tf+Oe7+XB2cblfgLjNrCOwD/NXdH4um1s+J/sK6HfiXmc0lTJoZaWYO9HT3ubWo/zTgDHf/Vh3Uv72Sr78B8EB0/eOAMnefCFxkZscSZvUuBc6MrdocSLn2rb77WIvLsiL63ncGHon+vdkIuNfdnzKzMRDv73zdIUAkT5nZ6qSXzQnLHWyOXp/r7vfkvqr8UcfhLPm8GQeuTI+NJlh8DJREA8pFRKqkljORPOXuLSueVxdEzKyR/sKXfKH/HkW2n8acidQzFlbpLzezn5rZIuBOM9vBzB6zsJL3suh5p6T3TLZoZW8LK36/bGbXRMd+bGZH1vLY7mY2xcxWmdlzZnZjmsHUFcdmUuNvzOyV6HzPJE82MLNRZjbfzJaY2S+r+fkMMrNFURddxbbvmNlb0fOBZjbVzJab2UIzu8HMGldxrn+a2W+TXl8eveczMzsr5dijLKyyvtLMFpjZ2KTdU6I/l5vZajMbXPGzTXr/EDObbuEODNPNbEimP5tt/DnvaGZ3RtewzMwmJO07zsJK6SvN7EMzGxFtn2dmhyUdN7biezazbmbmZna2mX0CTIq2Pxh9Dyui/0Z6J72/mZn9X/R9roj+G2tmZo+b2Y9SructC93sIkVD4UykftoF2BHoCowm/L98Z/S6C2E82A3VvH8Q8B7QHvgTcLtZlYtqVnfsvcDrhDWfxhLGrFUlkxpPJYzp2AloDPwEwMz2JqzMPQrYLfq8TqQRrc+1Bjgk5bz3Rs83A5dE1zMYOBQ4v5q6iWoYEdVzOGG18MNSDlkDnA60JSzqeZ6ZHR/tq5ie39bdW7r71JRz7wg8DlwfXdu1wONmlryWVtqfTRo1/Zz/Regm7x2d6y9RDQMJdwa5PLqGYcC8qn4eaRwE7AUcEb1+kvBz2omwBEtyN/w1hFl/Qwj/HVess3UX8PWAdDPrS1hT6vFtqEOk3lM4E6mftgBXuft6d/8qmu7+sLuvjVb3/x3hL8uqzHf3W919M+EvxF0Jg2MzPtbMugD7A7929w3u/jJhZlNaGdZ4p7u/7+5fAQ8A/aLtJwKPufuUaFr/lVS/aOZ9wCkAFmZjfTvahrvPcPdp7r7J3ecBN6epI53vRfW94+5rCGE0+fomu/vb7r7F3d+KPi+T80IIcx+4+7+iuu4jzJY8JumYqn42lVT3c7Yw8+5IYIy7L3P3je7+YvTWs4E73P3Z6Bo+dffUGavVGevua6L6cPc73H1V9H2NBfqaWRszawCcBVwcfcZmd381Om4i0MvMekbnHEW4ddCGbahDpN5TOBOpnxa7+7qKF2bW3MxujrqJVhK60domd+2lWFTxxN3XRk9bbuOxuwFLk7ZB5ZW0K8mwxkVJz9cm1bRb8rmjcFTdEh73At81sybAd4E33H1+VEevqKtvUVTH/yO0otWkUg3A/JTrG2ThxvGLzWwFMCbD81ace37KtvlUXom8qp9NJTX8nDsTvrNlad7aGfgww3rT+fpnY+FuA3+IukZXkmiBax89mqb7rOi/6fuB70ch7hRCS59IUVE4E6mfUqdZXwbsAQxy99YkutGyef+/hcCOZtY8aVt1t3bZnhoXJp87+swqb5/j4Wbt8wmtRMldmhC6R+cQZlm2JiwXss01EC0xkuReQstPZ3dvA/wj6bw1TYv/jNANmawLtVvssrqf8wLCd9Y2zfsWAN+o4pxrCF2hFXZJc0zyNZ5KuCfhYYQFW7sl1fAlsK6az7oLOI3Q3bw2tQtYpBgonIkUhlaEsUXLo/FLV2X7A6OWqDJgrJk1NrPBVO6Gq8saHwKONrNvRoP3x1Hz7697gYsJ4eTBlDpWAqvNbE/gvAxreAA408z2jsJhav2tCK1S66LxW6cm7VtM6IbtUcW5nyB0551qZo3M7GRgb+CxDGtLrSPtzzlaPPNJ4KZo4kCJJW5XczvwAzM71MwamFnH6OcDMJOwrluJmZUSuplrqmE9oXWzOaF1sqKGLcAdwLVmtlvUyjY4auUkCmNbgP9DrWZSpBTORArDdUAzQqvENOCpHH3uaYRB9UuA3xK6pNZXcWyta3T3WcAFhMC1kLBAbnkNb6sY8zXJ3b9M2v4TQnBaBdwa1ZxJDU9G1zCJsCDlpJRDzgfGmdkq4NeEMFfx3rWEsV+vWJglekDKuZcARxNavZYQBsgfnVJ3pmr6OY8CNhJaD78AfhzV8DphwsFfCDf2fpFEa96VhJauZcDVVG6JTOduQsvlp8DsqI5kPwHeBqYTFjb9I5X/PrqbsBhw2pm/IoVOi9CKSJ0xs/sJK+pnveVOCpeZnQ6Mdvdvxl2LSBzUciYitWZm+5vZN6JusBGEcUYTanqfSFWiLuPzgVvirkUkLgpnIrI9dgEmA6sJa3Sd5+5vxlqR1FtmdgRhfN7n1Nx1KlKw1K0pIiIikkfUciYiIiKSRxTORERERPJIo7gLqCvt27f3bt26xV2GiIiISI1mzJjxpbt3SLevYMJZt27dKCsri7sMERERkRqZWeot276mbk0RERGRPKJwJiIiIpJHFM5ERERE8kjBjDkTERER2S7z5sHkydC8OXzve7GVoXAmIiIixakijFU85kdj9A8/XOFMREREJOuqCmPt28Pw4XD55eHPvfaKq0JA4UxEREQK1baEsQb5Mwxf4UxEREQKQz0NY6kUzkRERKR+KpAwlkrhTEREROqH6sLYQQfBT34Swtjee9erMJZK4UxERETyU5GEsVQKZyIiIpIfijSMpVI4ExERkXjMmwcvvpgIY/Pmhe1FFsZSKZyJiIhIbtQUxi67rCjDWCqFMxEREckOhbFaUTgTERGRujF/fuUxYwpjtZLVcGZmI4C/Ag2B29z9Dyn7mwB3AwOAJcDJ7j7PzE4DLk86dF9gP3efmc16RUREZBtUFcbatQshTGGsVrIWzsysIXAjcDhQDkw3s4nuPjvpsLOBZe6+u5mNBP5ICGj3APdE59kHmKBgJiIiEjOFsZzIZsvZQGCuu38EYGbjgeOA5HB2HDA2ev4QcIOZmbt70jGnAOOzWKeIiIikozAWi2yGs47AgqTX5cCgqo5x901mtgJoB3yZdMzJhBC3FTMbDYwG6NKlS91ULSIiUqwUxvJCXk8IMLNBwFp3fyfdfne/BbgFoLS01NMdIyIiIlVQGMtL2QxnnwKdk153iralO6bczBoBbQgTAyqMBO7LYo0iIiLFQ2GsXshmOJsO9DSz7oQQNhI4NeWYicAZwFTgRGBSxXgzM2sAfA84MIs1ioiIFC6FsXopa+EsGkN2IfA0YSmNO9x9lpmNA8rcfSJwO/AvM5sLLCUEuArDgAUVEwpERESkBgpjBcEqT4ysv0pLS72srCzuMkRERHKnpjBW8VAYyztmNsPdS9Pty+sJASIiIpKkujB20EFw6aUhjPXurTBWjymciYiI5CuFsaKkcCYiIpIvFMYEhTMREZH4zJ8PL76YCGMffxy2K4wVNYUzERGRXKkpjF1yicKYKJyJiIhkjcKY1ILCmYiISF355JPKY8YUxqQWFM5ERERqS2FMskDhTEREJFMKY5IDCmciIiJVURiTGCiciYiIrF4N778Pc+YkHmVlCmMSC4UzEREpDu6wcGHlAFbxWLAgcVyDBtCjB/TvrzAmsVA4ExGRwrJhA8ydmz6ErVqVOK5lS9hzz9Aitueeicfuu0OTJvHVL0VP4UxEROqnpUvTB7CPPoLNmxPHdeoUQtcZZ1QOYbvtBmbx1S9SBYUzERHJX5s3h4Vc04WwxYsTxzVuDL16Qd++cPLJiQDWqxe0ahVf/SK1oHAmIiLxW7MG3ntv6wD2/vuwfn3iuPbtQ+g67rjKrWDdukHDhrGVL1KXFM5ERCQ3tnVA/p57whFHJALYHnuEcCZS4BTORESkbmlAvsh2yWo4M7MRwF+BhsBt7v6HlP1NgLuBAcAS4GR3nxft2xe4GWgNbAH2d/d12axXRES2gQbki2RF1sKZmTUEbgQOB8qB6WY20d1nJx12NrDM3Xc3s5HAH4GTzawR8G9glLv/z8zaARuzVauIiFRh8+awSv6cOfDuuxqQL5ID2Ww5GwjMdfePAMxsPHAckBzOjgPGRs8fAm4wMwO+Bbzl7v8DcPclWaxTRETWrNl6hfyKAfnrkjot2rWDvfbSgHyRLMpmOOsIJI3wpBwYVNUx7r7JzFYA7YBegJvZ00AHYLy7/yn1A8xsNDAaoEuXLnV+ASIiBcUdFi1K3xX5ySeJ4xo0gO7dQ+g6/PDKIUwD8kWyLl8nBDQCvgnsD6wFnjezGe7+fPJB7n4LcAtAaWmp57xKEZF8tGEDfPhh+hC2cmXiuBYtQuA68MCtB+Q3bRpf/SJFLpvh7FOgc9LrTtG2dMeUR+PM2hAmBpQDU9z9SwAzewLYD3geEREJli5NvzbYhx9WHpDfsWMIXaNGVQ5hHTtqQL5IHspmOJsO9DSz7oQQNhI4NeWYicAZwFTgRGCSu1d0Z15hZs2BDcBBwF+yWKuISH5KHpCf+vjii8RxjRtDz56wzz5w0kmV1wbTgHyReiVr4SwaQ3Yh8DRhKY073H2WmY0Dytx9InA78C8zmwssJQQ43H2ZmV1LCHgOPOHuj2erVhGR2GU6IH/HHcOA/GOO2XpAfqN8HakiItvC3KsfqmVmxwCPu/uW3JRUO6WlpV5WVhZ3GSIiVavNgPzUhwbkixSEaCx9abp9mfwz62TgOjN7mND6NadOqxMRKTQakC8i26HGcObu3zez1sApwD/NzIE7gfvcfVX17xYRKWDLlqUPYBqQLyLbIaMBCu6+0sweApoBPwa+A1xuZte7+9+yWaCISKwyHZBfUhJWw9eAfBHZTjWGMzM7FvgBsDvhPpgD3f2LaCblbEDhTEQKy6JFMHEiTJgAL7ygAfkiklOZ/CY5AfiLu09J3ujua83s7OyUJSKSY++/H8LYhAkwbVoYvN+jB/zwh7DvvhqQLyI5k0k4GwssrHhhZs2And19XuqK/SIi9caWLVBWlghk774btg8YAOPGwfHHQ+/eGhMmIjmXSTh7EBiS9HpztG3/rFQkIpItGzbA5MkhjP33v/DZZ+Fm3cOHw/nnw7HHgu7TKyIxyyScNXL3DRUv3H2DmTXOYk0iInVn5Up46qkQyB5/PLxu3hyOPDK0jn3722EcmYhInsgknC02s2OjFf0xs+OAL7NblojIdkge0P/886HFrEOHMIvy+OPh0EOhWbO4qxQRSSuTcDYGuMfMbgAMWACcntWqRES2VVUD+n/0oxDIBg8OXZgiInkuk0VoPwQOMLOW0evVWa9KRKQmGtAvIgUqo0V5zOwooDfQ1KJfdO4+Lot1iYhsTQP6RaQIZLII7T+A5sDBwG3AicDrWa5LRCTQgH4RKTKZtJwNcfd9zewtd7/azP4PeDLbhYlIEdOAfhEpYpmEs4r7lqw1s92AJcCu2StJRIqSBvSLiACZhbNHzawt8GfgDcCBW7NalYgUvqoG9O+3H1x9dQhkffpoQL+IFJ1qw5mZNQCed/flwMNm9hjQ1N1X5KQ6ESksVQ3oP+ggOO88OO44DegXkaJXbThz9y1mdiPQP3q9Hlif6cnNbATwV6AhcJu7/yFlfxPgbmAAobv0ZHefZ2bdgHeB96JDp7n7mEw/V0TyyKpV8OSTWw/oHzEitI4ddZQG9IuIJMmkW/N5MzsB+I+7e6YnNrOGwI3A4UA5MN3MJrr77KTDzgaWufvuZjYS+CNwcrTvQ3fvl+nniUgeWbQIHn00BLLnngstZu3bw4knhkB22GEa0C8iUoVMwtm5wKXAJjNbR7hLgLt76xreNxCY6+4fAZjZeOA4IDmcHQeMjZ4/BNxgpgEmIvXSBx8kxo9NnZoY0H/hhSGQDRmiAf0iIhnI5A4BrWp57o6EWz1VKAcGVXWMu28ysxVAu2hfdzN7E1gJ/MrdX6plHSKSDVu2wIwZiUA2O/p3lwb0i4hsl0wWoR2Wbru7T6n7cr62EOji7kvMbAAwwcx6u/vKlNpGA6MBumgQsUj2bdgAL76YGND/6aeJAf1jxmhAv4hIHcikW/PypOdNCd2VM4BDanjfp0DnpNedom3pjik3s0ZAG2BJNLZtPYC7zzCzD4FeQFnym939FuAWgNLS0ozHw4nINli1qvIK/StWaEC/iEgWZdKteUzyazPrDFyXwbmnAz3NrDshhI0ETk05ZiJwBjCVcFuoSe7uZtYBWOrum82sB9AT+CiDzxSRulDVgP4TTtCAfhGRLMvoxucpyoG9ajooGkN2IfA0YSmNO9x9lpmNA8rcfSJwO/AvM5sLLCUEOIBhwDgz2whsAca4+9Ja1CoimdKAfhGRvGA1rY5hZn8j3BUAoAHQD5jn7t/Pcm3bpLS01MvKymo+UESC6gb0H3+8BvSLiGSRmc1w99J0+zJpOUtOPJuA+9z9lTqpTERySwP6RUTyXibh7CFgnbtvhrC4rJk1d/e12S1NROqEBvSLiNQrGd0hADgMWB29bgY8AwzJVlEisp0+/xwmTtSAfhGReiiTcNbU3SuCGe6+2syaZ7EmEamNdAP6u3fXgH4RkXomk3C2xsz2c/c3AKJFYb/KblkiUiN3KCvTCv0iIgUmk3D2Y+BBM/uMcF/NXUjcnFxEcqmmAf3HHgtdu8ZdpYiIbIdMFqGdbmZ7AntEm95z943ZLUtEvqYB/SIiRSWTe2teANzj7u9Er3cws1Pc/aasVydSrDSgX0SkaGXSrflDd7+x4oW7LzOzHwIKZyJ16YMPQlflhAnw6qsa0C8iUqQyCWcNzcyim5FjZg2BxtktS6QIuFdeoX/WrLBdA/pFRIpaJuHsKeB+M7s5en0u8GT2ShIpYBs3Jgb0T5iQGNA/bBiMHh1W6NeAfhGRopZJOPspMBoYE71+izBjU0QysWoVPP10CGOPPVZ5QP9xx4UB/e3axV2liIjkiUxma24xs9eAbwDfA9oDD2e7MJF67fPP4dFHEwP616/XgH4REclIleHMzHoBp0SPL4H7Adz94NyUJlLPzJ2b6K5MHtB/wQUa0C8iIhmrruVsDvAScLS7zwUws0tyUpVIfaAB/SIikgXVhbPvAiOBF8zsKWA84Q4BIsUreUD/f/8L5eUa0C8iInWqynDm7hOACWbWAjiOcBunnczs78Aj7v5MjmoUidfatfDEE4kV+pcvD+PFRoyA3/1OA/pFRKROZTIhYA1wL3Cvme0AnESYwalwJoVr/foww3L8+LBS/5o1IYB95zuJAf3Nm8ddpYiIFKBMltL4mrsvA26JHjUysxHAX4GGwG3u/oeU/U2Au4EBwBLgZHefl7S/CzAbGOvu12xLrSLbbNMmmDQpBLL//CcsedGuHXz/+3DyyXDggdBom/6XERER2WZZ+5smupPAjcDhQDkw3cwmuvvspMPOBpa5++5mNhL4I3By0v5r0YK3kk1btsArr8B998FDD8HixdC6dWghGzkSDj0USkrirlJERIpINpsBBgJz3f0jADMbTxi7lhzOjgPGRs8fAm6ouFWUmR0PfAysyWKNUozcoawstJDdf39Ypb9ZMzj22BDIRoyApk3jrlJERIpUNsNZR2BB0utyYFBVx7j7JjNbAbQzs3WEcW2HAz/JYo1SLNzhnXdCIBs/Hj76CBo3hiOPhGuugaOPhpYt465SREQkq+Fse4wF/uLuq62aNaLMbDTh1lJ06dIlN5VJ/fL++6F1bPx4mD07LHtx2GFw5ZVhYH/btnFXKCIiUkk2w9mnQOek152ibemOKTezRkAbwsSAQcCJZvYnoC2wxczWufsNyW92968nJ5SWlnpWrkLqn/nz4YEHQiB7442wCOywYfD3v4fbJ3XoEHeFIiIiVcpmOJsO9DSz7oQQNhI4NeWYicAZwFTgRGCSuztwYMUBZjYWWJ0azEQqWbQIHnwwBLJXXw3bBg2Cv/wFTjoJOnaMtz4REZEMZS2cRWPILgSeJiylcYe7zzKzcUCZu08Ebgf+ZWZzgaWEACeSmSVLwpIX48fD5Mlh5mXfvvD738P3vgc9esRdoYiIyDaz0FBV/5WWlnpZWVncZUi2rVwZbps0fjw880xYm6xXLzjllLAW2V57xV2hiIhIjcxshruXptuXrxMCRBLWrg23TRo/Pvy5fn24f+Vll4WlL/r21c3FRUSkYCicSX5avz60jI0fH1rK1qyBXXaBMWNCIBs0SIFMREQKksKZ5I9Nm+CFFxK3T1q+PHH7pJEjw+2TGjaMu0oREZGsUjiTeFXcPmn8+DDbcvFiaNUq3D7plFN0+yQRESk6CmeSe1XdPumYY0IL2ZFH6vZJIiJStBTOJHfefrvy7ZNKSkIQ+/OfQzDT7ZNEREQUziTLPvggEcgqbp906KHwq1+F2yftsEPcFYqIiOQVhTOpe598krifZcXtkw48EG66Kdw+aaed4q5QRP5/e/cePNF3gAAAD+VJREFUHFWZ5nH8+xAwAZNFjXKRgMBAgSOXECJIHC28bbErhUUNCsi6XkfEC8qsyyAzzrqWYw1bjroZdbfwisysYMmCjIU6oLhe2EGDgnJTopUdw8jFZORSGEjIs3+cQxJDRzqQ7tPp/n2qUjl9zunO8+SlmifnvP0+IpKyVJxJ24jVPmnUKHj44aB9UkFBtPGJiIi0EyrO5PgdaZ+0eHGwBEZ9PQwbBg8+GKzWr/ZJIiIirabiTFpn715Yvjy4Qvb668HaZAMHBnPIJk+GH/4w6ghFRETaNRVncmwHDsCKFY3tk2pqoE8f+OlPg6UvCgu1Wr+IiEgbUXEmsR069N32Sfv3B+2Tbr65sX1Shw5RRykiIpJ2VJxJo7o6eOutoCBbsiRon3TaaXD11UFBduGFap8kIiKSYCrOMl19ffDpyiPtk3btamyfNGUKXHqp2ieJiIgkkYqzTOQO69Y1tk+qrAzaJ40fH/SzVPskERGRyKg4yyQbNzau1v/558EVsXHjYN68oH1SXl7UEYqIiGQ8FWfpbtu2xtX6N20KJvFfcgnMnRvculT7JBERkZSS0OLMzMYB/w5kAU+5+6+bHc8GngdGAlXAZHevMLNRwPwjpwH3ufvSRMaaVv78Z3jxxaAgW7cu2HfBBfD44zBpktoniYhkmNraWiorK6mpqYk6lIyTk5NDQUEBnVoxfzthxZmZZQGPA5cBlcAHZrbc3Tc3Oe1G4K/uPsDMpgDzgMnARqDY3evMrCewwcz+4O51iYq33du5s7F90nvvBfvOPRd+8xu46iq1TxIRyWCVlZXk5eXRt29fTOtSJo27U1VVRWVlJf369Yv7eYm8cjYKKHf3LwDMbBFwBdC0OLsCuC/cfgl4zMzM3Q80OScH8ATG2X5VVwftkxYtamyfNHQo/OpXwWr9P/hB1BGKiEgKqKmpUWEWATMjPz+f3bt3t+p5iSzOegFfNnlcCYxu6ZzwKtkeIB/42sxGA88AZwHXxLpqZmY3AzcD9OnTp80TSEn79gWLwjZtnzRgAPz850FBds45UUcoIiIpSIVZNI7n956yS7y7+1p3Pwc4F7jHzI5a28Hd57t7sbsXn3HGGckPMlm+/RZeeqlxvtg118DHH8OsWcGcss8+g/vvV2EmIiIpqaqqisLCQgoLC+nRowe9evVqeHzo0KHvfW5ZWRkzZ8485s8oKSlpq3Ajl8grZ9uB3k0eF4T7Yp1TaWYdga4EHwxo4O5bzGw/MAQoS1y4KSZW+6Tu3eEnPwkWhz3vPLVPEhGRdiE/P5/169cDcN9995Gbm8vdd9/dcLyuro6OHWOXJMXFxRQXFx/zZ6xZs6Ztgk0Bifzf/QNgoJn1M7OTgCnA8mbnLAeuDbcnAW+6u4fP6QhgZmcBg4GKBMaaGurqYNWqoADr0SNYe2zFimBh2DfegO3bobQUSkpUmImISLt23XXXccsttzB69Ghmz57N+++/z5gxYxgxYgQlJSV8+umnALz11luMHz8eCAq7G264gbFjx9K/f39KS0sbXi83N7fh/LFjxzJp0iQGDx7MtGnTcA+mrq9YsYLBgwczcuRIZs6c2fC6TVVUVHDBBRdQVFREUVHRd4q+efPmMXToUIYPH86cOXMAKC8v59JLL2X48OEUFRXx+eefn/DvJmFXzsI5ZLcDrxMspfGMu28ys/uBMndfDjwNLDSzcqCaoIAD+BEwx8xqgXrgVnf/OlGxRipW+6Tc3O+2TzrppKijFBGRdHHXXRBexWozhYXw6KOtflplZSVr1qwhKyuLvXv38s4779CxY0dWrVrF3LlzWbJkyVHP2bp1K6tXr2bfvn0MGjSIGTNmHLVMxUcffcSmTZs488wzOf/883nvvfcoLi5m+vTpvP322/Tr14+pU6fGjKlbt26sXLmSnJwctm3bxtSpUykrK+PVV1/l5ZdfZu3atXTp0oXq6moApk2bxpw5c5g4cSI1NTXU19e3+vfQXELXOXP3FcCKZvt+2WS7BrgyxvMWAgsTGVuk3OHDD+GFFxrbJ+XkBFfKpkwJ2id17hx1lCIiIgl15ZVXkpWVBcCePXu49tpr2bZtG2ZGbW1tzOdcfvnlZGdnk52dTbdu3di5cycFzZaLGjVqVMO+wsJCKioqyM3NpX///g1LWkydOpX58+cf9fq1tbXcfvvtrF+/nqysLD777DMAVq1axfXXX0+XLl0AOO2009i3bx/bt29n4sSJQLCmWVtQh4Bk2rSpsX1SebnaJ4mISPIdxxWuRDn55JMbtu+9914uuugili5dSkVFBWPHjo35nOzs7IbtrKws6uqOXgI1nnNa8sgjj9C9e3c2bNhAfX19mxVcraGJS4lWXh6sOzZ0KAwZAg8+CP36wdNPBwvHLl8OV1+twkxERDLanj176NWrFwDPPfdcm7/+oEGD+OKLL6ioqABg8eLFLcbRs2dPOnTowMKFCzl8+DAAl112Gc8++ywHDgRLsVZXV5OXl0dBQQHLli0D4ODBgw3HT4SKs0T48stgZf5zz4WBA+EXv4BTToHHHoO//CX4FOYNN6ivpYiISGj27Nncc889jBgxolVXuuLVuXNnnnjiCcaNG8fIkSPJy8uja9euR5136623smDBAoYPH87WrVsbru6NGzeOCRMmUFxcTGFhIQ899BAACxcupLS0lGHDhlFSUsKOHTtOOFY78gmG9q64uNjLyiJcaWPnzmAtskWL4N13jwQVzCG76iro3fv7ny8iIpIgW7Zs4eyzz446jMjt37+f3Nxc3J3bbruNgQMHMmvWrIT/3Fi/fzNb5+4x1wjRnLMTUV0NS5cGBdmbbwafvBwyBB54IFitf8CAqCMUERGR0JNPPsmCBQs4dOgQI0aMYPr06VGHFJOKs9baty+YJ3akfVJtbVCEzZ0bFGRDhkQdoYiIiMQwa9aspFwpO1EqzuK1YwfccQe88grU1AS3Ke+8M7htWVQE6lkmIiIibUDFWbxOPRU2boSbbgoKsjFjtEq/iIi0G+6u5ucROJ65/SrO4pWdDZs36wqZiIi0Ozk5OVRVVZGfn68CLYncnaqqqlavlabirDX0D1pERNqhgoICKisr2b17d9ShZJycnJyjOhgci4ozERGRNNepU6eGtkWS+jRpSkRERCSFqDgTERERSSEqzkRERERSSNq0bzKz3cD/JeFHnQ58nYSfk4oyOXfI7PyVe+bK5PwzOXfI7PyTkftZ7n5GrANpU5wli5mVtdQLK91lcu6Q2fkr98zMHTI7/0zOHTI7/6hz121NERERkRSi4kxEREQkhag4a735UQcQoUzOHTI7f+WeuTI5/0zOHTI7/0hz15wzERERkRSiK2ciIiIiKUTFWQxm9oyZ7TKzjS0cNzMrNbNyM/vYzIqSHWOixJH7WDPbY2brw69fJjvGRDGz3ma22sw2m9kmM7szxjnpPPbx5J+W429mOWb2vpltCHP/1xjnZJvZ4nDs15pZ3+RHmhhx5n+dme1uMvY3RRFrophZlpl9ZGavxDiWtmMPx8w93ce9wsw+CXMri3E8kvd89daM7TngMeD5Fo7/HTAw/BoN/Ef4PR08x/fnDvCOu49PTjhJVQf8k7t/aGZ5wDozW+num5uck85jH0/+kJ7jfxC42N33m1kn4F0ze9Xd/9TknBuBv7r7ADObAswDJkcRbALEkz/AYne/PYL4kuFOYAvwNzGOpfPYw/fnDuk97gAXuXtLa5pF8p6vK2cxuPvbQPX3nHIF8LwH/gScYmY9kxNdYsWRe9py96/c/cNwex/Bm1WvZqel89jHk39aCsdzf/iwU/jVfELuFcCCcPsl4BIzsySFmFBx5p+2zKwAuBx4qoVT0nbs48g900Xynq/i7Pj0Ar5s8riSDPlPLDQmvP3xqpmdE3UwiRDethgBrG12KCPG/nvyhzQd//DWznpgF7DS3Vsce3evA/YA+cmNMnHiyB/gx+GtnZfMrHeSQ0ykR4HZQH0Lx9N57I+VO6TvuEPwR8gfzWydmd0c43gk7/kqzqS1PiRoOTEc+C2wLOJ42pyZ5QJLgLvcfW/U8STbMfJP2/F398PuXggUAKPMbEjUMSVTHPn/Aejr7sOAlTReSWrXzGw8sMvd10UdS7LFmXtajnsTP3L3IoLbl7eZ2YVRBwQqzo7XdqDpXw8F4b605+57j9z+cPcVQCczOz3isNpMON9mCfB7d//vGKek9dgfK/90H38Ad/8GWA2Ma3aoYezNrCPQFahKbnSJ11L+7l7l7gfDh08BI5MdW4KcD0wwswpgEXCxmf2u2TnpOvbHzD2Nxx0Ad98eft8FLAVGNTslkvd8FWfHZznwj+GnOM4D9rj7V1EHlQxm1uPIXAszG0Xwbygd3qQI83oa2OLuD7dwWtqOfTz5p+v4m9kZZnZKuN0ZuAzY2uy05cC14fYk4E1Pk4Ui48m/2TybCQRzEts9d7/H3QvcvS8whWBc/6HZaWk59vHknq7jDmBmJ4cffsLMTgb+Fmi+UkEk7/n6tGYMZvYCMBY43cwqgX8hmCCLu/8nsAL4e6AcOABcH02kbS+O3CcBM8ysDvgWmJIOb1Kh84FrgE/CuTcAc4E+kP5jT3z5p+v49wQWmFkWQcH5oru/Ymb3A2XuvpygcF1oZuUEH5qZEl24bS6e/Gea2QSCT/VWA9dFFm0SZNDYHyWDxr07sDT8e7Mj8F/u/pqZ3QLRvuerQ4CIiIhICtFtTREREZEUouJMREREJIWoOBMRERFJISrORERERFKIijMRERGRFKLiTETSmpkdNrP1Tb7mtOFr9zWz5usiiYicEK1zJiLp7tuwLZGISLugK2cikpHMrMLM/s3MPjGz981sQLi/r5m9GTZ6fsPM+oT7u5vZ0rDp+wYzKwlfKsvMnjSzTWb2x3CFfcxsppltDl9nUURpikg7pOJMRNJd52a3NSc3ObbH3YcCjwGPhvt+CywIGz3/HigN95cC/xM2fS8CNoX7BwKPu/s5wDfAj8P9c4AR4evckqjkRCT9qEOAiKQ1M9vv7rkx9lcAF7v7F2HD9x3unm9mXwM93b023P+Vu59uZruBgiZNoDGzvsBKdx8YPv4Z0MndHzCz14D9wDJg2ZGG8SIix6IrZyKSybyF7dY42GT7MI1zeS8HHie4yvaBmWmOr4jERcWZiGSyyU2+/2+4vYbGxtbTgHfC7TeAGQBmlmVmXVt6UTPrAPR299XAz4CuwFFX70REYtFfciKS7jqb2fomj19z9yPLaZxqZh8TXP2aGu67A3jWzP4Z2A1cH+6/E5hvZjcSXCGbAXzVws/MAn4XFnAGlLr7N22WkYikNc05E5GMFM45K3b3r6OORUSkKd3WFBEREUkhunImIiIikkJ05UxEREQkhag4ExEREUkhKs5EREREUoiKMxEREZEUouJMREREJIWoOBMRERFJIf8PEvEL/BBsLt8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI09gonlGkqd"
      },
      "source": [
        "class CustomModel(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocabulary_size,\n",
        "                 embedding_dimensions=128,\n",
        "                 cnn_filters=50,\n",
        "                 dnn_units=512,\n",
        "                 model_output_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name=\"custom_test_model\"):\n",
        "        super(CustomModel, self).__init__(name=name)\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocabulary_size,\n",
        "                                          embedding_dimensions)\n",
        "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=2,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=3,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=4,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.last_dense = layers.Dense(units=model_output_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        l = self.embedding(inputs)\n",
        "        l_1 = self.cnn_layer1(l) \n",
        "        l_1 = self.pool(l_1) \n",
        "        l_2 = self.cnn_layer2(l) \n",
        "        l_2 = self.pool(l_2)\n",
        "        l_3 = self.cnn_layer3(l)\n",
        "        l_3 = self.pool(l_3) \n",
        "        \n",
        "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
        "        concatenated = self.dense_1(concatenated)\n",
        "        concatenated = self.dropout(concatenated, training)\n",
        "        model_output = self.last_dense(concatenated)\n",
        "        \n",
        "        return model_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AkZEswgdiUh"
      },
      "source": [
        "class BertCustomModel(TFBertPreTrainedModel):\n",
        "    \n",
        "    def __init__(self, config,\n",
        "                 cnn_filters=50,\n",
        "                 dnn_units=512,\n",
        "                 model_output_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name=\"custem_bert_model\", *inputs, **kwargs):\n",
        "        super().__init__(config, *inputs, **kwargs)\n",
        "        \n",
        "        self.bert = TFBertMainLayer(config, name=\"bert\", trainable = False)\n",
        "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
        "                                kernel_size=2,\n",
        "                                padding=\"valid\",\n",
        "                                activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        self.dense_1 = layers.Dense(units=256, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.last_dense = layers.Dense(units=model_output_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training=False, **kwargs):\n",
        "        bert = self.bert(inputs, training=training, **kwargs)\n",
        "        l = self.cnn_layer1(bert[0])\n",
        "        l = self.pool(l)\n",
        "        l = self.dense_1(l)\n",
        "        l = self.dropout(l, training)\n",
        "        model_output = self.last_dense(l) \n",
        "        \n",
        "        return model_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZghZTD8EBKu"
      },
      "source": [
        "class BertCustomModel2(TFBertPreTrainedModel):\n",
        "    \n",
        "    def __init__(self, config,\n",
        "                 cnn_filters=50,\n",
        "                 dnn_units=512,\n",
        "                 model_output_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name=\"custem_bert_model2\", *inputs, **kwargs):\n",
        "        super().__init__(config, *inputs, **kwargs)\n",
        "        self.bert = TFBertMainLayer(config, name=\"bert\", trainable = False)\n",
        "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=2,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=3,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=4,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.last_dense = layers.Dense(units=model_output_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training=False, **kwargs):\n",
        "        bert = self.bert(inputs, training=training, **kwargs)\n",
        "        l_1 = self.cnn_layer1(bert[0]) \n",
        "        l_1 = self.pool(l_1) \n",
        "        l_2 = self.cnn_layer2(bert[0]) \n",
        "        l_2 = self.pool(l_2)\n",
        "        l_3 = self.cnn_layer3(bert[0])\n",
        "        l_3 = self.pool(l_3) \n",
        "        \n",
        "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
        "        concatenated = self.dense_1(concatenated)\n",
        "        concatenated = self.dropout(concatenated, training)\n",
        "        model_output = self.last_dense(concatenated)\n",
        "        \n",
        "        return model_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVnOdcFgnCxh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4e248711-d85f-479b-c6eb-a8e14aeb26c2"
      },
      "source": [
        "no_folds = 5\n",
        "kfold = KFold(no_folds, shuffle=True)\n",
        "\n",
        "# Convert to numpy arrays so indexing with list of indexes works\n",
        "X = np.array(X) \n",
        "y = np.array(y)\n",
        "\n",
        "test_accuracy_log = []\n",
        "\n",
        "for train, test in kfold.split(X, y):\n",
        "  X_train = X[train]\n",
        "  X_train, vocab_len = BertTokenize(X_train)\n",
        "  y_train = y[train]\n",
        "  X_test = X[test]\n",
        "  X_test, _ = BertTokenize(X_test)\n",
        "  y_test = y[test]\n",
        "  #y_train = to_categorical(y_train, OUTPUT_CLASSES)\n",
        "  #y_test = to_categorical(y_test, OUTPUT_CLASSES)\n",
        "\n",
        "  custom_model = BertCustomModel2.from_pretrained('bert-base-uncased',\n",
        "                      cnn_filters=CNN_FILTERS,\n",
        "                      dnn_units=DNN_UNITS,\n",
        "                      model_output_classes=OUTPUT_CLASSES,\n",
        "                      dropout_rate=DROPOUT_RATE)\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "  custom_model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
        "\n",
        "  train_data = tf.data.Dataset.from_tensor_slices((tf.constant(X_train), tf.constant(y_train))).batch(12)\n",
        "  test_data = tf.data.Dataset.from_tensor_slices((tf.constant(X_test), tf.constant(y_test))).batch(12)\n",
        "  custom_model.fit(train_data, epochs=5)\n",
        "\n",
        "  score = custom_model.evaluate(test_data)\n",
        "  print(\"Results for fold\\n\\tTest accuracy: {:.4f}\\n\\tTest loss: {:.4f}\".format(score[1], score[0]))\n",
        "  test_accuracy_log.append(score[1])\n",
        "  \n",
        "print(f\"Accuracy score: {sum(test_accuracy_log) / len(test_accuracy_log)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing BertCustomModel2: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing BertCustomModel2 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertCustomModel2 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of BertCustomModel2 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dense_229', 'dense_228', 'dropout_780', 'conv1d_221', 'conv1d_220', 'conv1d_219', 'global_max_pooling1d_90']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "57/57 [==============================] - 635s 11s/step - loss: 4.0100 - accuracy: 0.1420\n",
            "Epoch 2/5\n",
            "57/57 [==============================] - 620s 11s/step - loss: 1.7110 - accuracy: 0.5633\n",
            "Epoch 3/5\n",
            "57/57 [==============================] - 620s 11s/step - loss: 1.5600 - accuracy: 0.5934\n",
            "Epoch 4/5\n",
            "57/57 [==============================] - 619s 11s/step - loss: 1.4514 - accuracy: 0.5952\n",
            "Epoch 5/5\n",
            "57/57 [==============================] - 617s 11s/step - loss: 1.3653 - accuracy: 0.6248\n",
            "15/15 [==============================] - 136s 9s/step - loss: 1.7500 - accuracy: 0.4970\n",
            "Results for fold\n",
            "\tTest accuracy: 0.4970\n",
            "\tTest loss: 1.7500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing BertCustomModel2: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing BertCustomModel2 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertCustomModel2 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of BertCustomModel2 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['conv1d_223', 'dropout_818', 'conv1d_222', 'dense_230', 'dense_231', 'conv1d_224', 'global_max_pooling1d_91']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "57/57 [==============================] - 634s 11s/step - loss: 3.2119 - accuracy: 0.1994\n",
            "Epoch 2/5\n",
            "57/57 [==============================] - 618s 11s/step - loss: 1.6469 - accuracy: 0.5926\n",
            "Epoch 3/5\n",
            "57/57 [==============================] - 620s 11s/step - loss: 1.4619 - accuracy: 0.6038\n",
            "Epoch 4/5\n",
            "57/57 [==============================] - 620s 11s/step - loss: 1.4158 - accuracy: 0.6107\n",
            "Epoch 5/5\n",
            "57/57 [==============================] - 618s 11s/step - loss: 1.3162 - accuracy: 0.6214\n",
            "15/15 [==============================] - 136s 9s/step - loss: 1.8780 - accuracy: 0.4852\n",
            "Results for fold\n",
            "\tTest accuracy: 0.4852\n",
            "\tTest loss: 1.8780\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing BertCustomModel2: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing BertCustomModel2 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertCustomModel2 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of BertCustomModel2 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dropout_856', 'dense_232', 'dense_233', 'conv1d_225', 'global_max_pooling1d_92', 'conv1d_226', 'conv1d_227']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "57/57 [==============================] - 636s 11s/step - loss: 2.7915 - accuracy: 0.2424\n",
            "Epoch 2/5\n",
            "57/57 [==============================] - 621s 11s/step - loss: 1.7342 - accuracy: 0.5378\n",
            "Epoch 3/5\n",
            " 8/57 [===>..........................] - ETA: 8:59 - loss: 1.2987 - accuracy: 0.6859"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-216-2ab5be593b02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0mcustom_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qV5SGaKVDnpO",
        "outputId": "8baa7d23-0208-48a7-f0f6-97fb563ff7b5"
      },
      "source": [
        "custom_model = BertCustomModel.from_pretrained('bert-base-uncased',\n",
        "                      cnn_filters=CNN_FILTERS,\n",
        "                      dnn_units=DNN_UNITS,\n",
        "                      model_output_classes=OUTPUT_CLASSES,\n",
        "                      dropout_rate=DROPOUT_RATE)\n",
        "#run_and_evaluate_bert(custom_model, X, y)\n",
        "custom_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing BertCustomModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing BertCustomModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertCustomModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of BertCustomModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['conv1d_191', 'global_max_pooling1d_80', 'dense_209', 'dense_208', 'dropout_474']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"bert_custom_model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  109482240 \n",
            "_________________________________________________________________\n",
            "conv1d_191 (Conv1D)          multiple                  153700    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_80 (Glo multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_208 (Dense)            multiple                  25856     \n",
            "_________________________________________________________________\n",
            "dropout_474 (Dropout)        multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_209 (Dense)            multiple                  4369      \n",
            "=================================================================\n",
            "Total params: 109,666,165\n",
            "Trainable params: 183,925\n",
            "Non-trainable params: 109,482,240\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoLSoLxvokxH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dc71d31-6b86-4ff6-f5f0-2e69d07bc8e4"
      },
      "source": [
        "custom_model = BertCustomModel2.from_pretrained('bert-base-uncased',\n",
        "                      cnn_filters=CNN_FILTERS,\n",
        "                      dnn_units=DNN_UNITS,\n",
        "                      model_output_classes=OUTPUT_CLASSES,\n",
        "                      dropout_rate=DROPOUT_RATE)\n",
        "run_and_evaluate_bert(custom_model, X, y, save_model=False)\n",
        "#custom_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing BertCustomModel2: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing BertCustomModel2 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertCustomModel2 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of BertCustomModel2 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['conv1d_204', 'dense_218', 'global_max_pooling1d_85', 'conv1d_205', 'conv1d_206', 'dense_219', 'dropout_664']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "57/57 [==============================] - 631s 11s/step - loss: 2.5605 - accuracy: 0.3306\n",
            "Epoch 2/5\n",
            "57/57 [==============================] - 616s 11s/step - loss: 2.0371 - accuracy: 0.4975\n",
            "Epoch 3/5\n",
            "57/57 [==============================] - 617s 11s/step - loss: 1.9040 - accuracy: 0.5239\n",
            "Epoch 4/5\n",
            "57/57 [==============================] - 615s 11s/step - loss: 1.8619 - accuracy: 0.5118\n",
            "Epoch 5/5\n",
            "57/57 [==============================] - 615s 11s/step - loss: 1.8499 - accuracy: 0.5261\n",
            "15/15 [==============================] - 135s 9s/step - loss: 2.0363 - accuracy: 0.4142\n",
            "Results:\n",
            "\tTest accuracy: 0.4142\n",
            "\tTest loss: 2.0363\n",
            "Accuracy score: 0.41420117020606995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4G31iUVKfSl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}